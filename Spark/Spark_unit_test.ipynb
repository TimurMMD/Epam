{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG8xBA-Ycq82",
        "outputId": "5ac1ebd7-2d50-4d39-a7f3-f9877ec61f2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pygeohash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOFVB-KAc205",
        "outputId": "f683501b-5f87-468f-b0d7-ed06902ffef1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygeohash\n",
            "  Downloading pygeohash-3.2.2-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Downloading pygeohash-3.2.2-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (43 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygeohash\n",
            "Successfully installed pygeohash-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "pytest.main([\"-v\", \"pyspark_testing.py\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9eFCgzHehTF",
        "outputId": "fdf0a098-2695-4826-b52d-813998cebe27"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================= test session starts ==============================\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.56, anyio-4.12.0, typeguard-4.4.4\n",
            "collecting ... collected 5 items\n",
            "\n",
            "pyspark_testing.py::test_geohash_udf_valid PASSED                        [ 20%]\n",
            "pyspark_testing.py::test_geohash_udf_none FAILED                         [ 40%]\n",
            "pyspark_testing.py::test_fill_missing_coordinates PASSED                 [ 60%]\n",
            "pyspark_testing.py::test_aggregate_weather PASSED                        [ 80%]\n",
            "pyspark_testing.py::test_join_restaurant_weather PASSED                  [100%]\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "____________________________ test_geohash_udf_none _____________________________\n",
            "\n",
            "spark = <pyspark.sql.session.SparkSession object at 0x7fd543f8cb60>\n",
            "\n",
            "    def test_geohash_udf_none(spark):\n",
            "        # Edge case: missing lat/lng\n",
            ">       df = spark.createDataFrame([Row(lat=None, lng=None)])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "pyspark_testing.py:38: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py:1599: in createDataFrame\n",
            "    return self._create_dataframe(\n",
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py:1643: in _create_dataframe\n",
            "    rdd, struct = self._createFromLocal(\n",
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py:1198: in _createFromLocal\n",
            "    struct = self._inferSchemaFromList(data, names=schema)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <pyspark.sql.session.SparkSession object at 0x7fd543f8cb60>\n",
            "data = [Row(lat=None, lng=None)], names = None\n",
            "\n",
            "    def _inferSchemaFromList(\n",
            "        self, data: Iterable[Any], names: Optional[List[str]] = None\n",
            "    ) -> StructType:\n",
            "        \"\"\"\n",
            "        Infer schema from list of Row, dict, or tuple.\n",
            "    \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : iterable\n",
            "            list of Row, dict, or tuple\n",
            "        names : list, optional\n",
            "            list of column names\n",
            "    \n",
            "        Returns\n",
            "        -------\n",
            "        :class:`pyspark.sql.types.StructType`\n",
            "        \"\"\"\n",
            "        if not data:\n",
            "            raise PySparkValueError(\n",
            "                errorClass=\"CANNOT_INFER_EMPTY_SCHEMA\",\n",
            "                messageParameters={},\n",
            "            )\n",
            "        infer_dict_as_struct = self._jconf.inferDictAsStruct()\n",
            "        infer_array_from_first_element = self._jconf.legacyInferArrayTypeFromFirstElement()\n",
            "        infer_map_from_first_pair = self._jconf.legacyInferMapStructTypeFromFirstItem()\n",
            "        prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n",
            "        schema = reduce(\n",
            "            _merge_type,\n",
            "            (\n",
            "                _infer_schema(\n",
            "                    row,\n",
            "                    names,\n",
            "                    infer_dict_as_struct=infer_dict_as_struct,\n",
            "                    infer_array_from_first_element=infer_array_from_first_element,\n",
            "                    infer_map_from_first_pair=infer_map_from_first_pair,\n",
            "                    prefer_timestamp_ntz=prefer_timestamp_ntz,\n",
            "                )\n",
            "                for row in data\n",
            "            ),\n",
            "        )\n",
            "        if _has_nulltype(schema):\n",
            ">           raise PySparkValueError(\n",
            "                errorClass=\"CANNOT_DETERMINE_TYPE\",\n",
            "                messageParameters={},\n",
            "            )\n",
            "E           pyspark.errors.exceptions.base.PySparkValueError: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py:1071: PySparkValueError\n",
            "=========================== short test summary info ============================\n",
            "FAILED pyspark_testing.py::test_geohash_udf_none - pyspark.errors.exceptions....\n",
            "========================= 1 failed, 4 passed in 33.49s =========================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.TESTS_FAILED: 1>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8KWlxOPJek7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}